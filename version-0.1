
# coding: utf-8

# In[1]:


from numpy import *
import numpy as np

training_data = [
({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'no'}, False),
({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'yes'}, False),
({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'no'}, True),
({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'no'}, True),
({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'no'}, True),
({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'yes'}, False),
({'level':'Mid', 'lang':'R', 'tweets':'yes', 'phd':'yes'}, True),
({'level':'Senior', 'lang':'Python', 'tweets':'no', 'phd':'no'}, False),
({'level':'Senior', 'lang':'R', 'tweets':'yes', 'phd':'no'}, True),
({'level':'Junior', 'lang':'Python', 'tweets':'yes', 'phd':'no'}, True),
({'level':'Senior', 'lang':'Python', 'tweets':'yes', 'phd':'yes'}, True),
({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, True),
({'level':'Mid', 'lang':'Java', 'tweets':'yes', 'phd':'no'}, True),
({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, False)
] 


# In[2]:


def get_attribute_list123(training_data):
    attri_list = []
    for t in training_data:
        if t[0].keys() not in attri_list:
            attri_list.append(t[0].keys())
    return attri_list
    # return a list containing values in this test attribute


# In[3]:


def counter_num_each_class(training_data):
    class_list = [t[1] for t in training_data]
    classes_set = []
    for c in class_list:
        if c not in classes_set:
            classes_set.append(c)
    num_each_class = []
    for c in classes_set:
        num_each_class.append(tuple((c,class_list.count(c))))
    return num_each_class
    # return a list of tuple(class, number of sample in this class)


# In[4]:


def most_common_class(training_data):
    current_most = 0
    for c in counter_num_each_class(training_data) :
        if c[1] > current_most:
            current_most = c[1]
            most_common_class = c[0]
    return most_common_class
    # return a string of the class name that most data fall in


# In[5]:


def values_in_test_attri(training_data, test_attri):
    value_list = []
    for t in training_data:
        if t[0][test_attri] not in value_list:
            value_list.append(t[0][test_attri])
    return value_list            
    # return a list containing values in this test attribute


# In[12]:


def get_attribute_list(training_data) :
    attri_list = []
    for t in training_data:
        for keys in t[0].keys():
            if keys not in attri_list:
                attri_list.append(keys)
    return attri_list


# In[13]:


def counter_helper(training_data):
    overall_class_list = [t[1] for t in training_data] #list with True or False of each tuples
    classes_set = []                               #list with two elements True and False in this case
    for c in overall_class_list:                   #Could be more depends on training data
        if c not in classes_set:
            classes_set.append(c)
                
    num_each_class = []                            #list with tuple: (True or False, Number of T ot F)  
    for c in classes_set:
        num_each_class.append(tuple((c,overall_class_list.count(c))))

    class_list = [t[0] for t in training_data]     #list of tuple of every attribute of its class with T of F
    attr_list = []                                 #example the first element in the list:('level', 'Senior', False)
    class_list_tuple1 = []                         #this is for calculation of information gain
    for i in range(len(class_list)):
        dic = class_list[i]
        for t in dic.items():
            class_list_tuple1.append(t+(overall_class_list[i],))
            if t not in attr_list:
                attr_list.append(t+(overall_class_list[i],))
                
    return class_list_tuple1


# In[14]:


def entropy(test_attri):                      #dump way to calculate information gain, take two test_attris: attribute name and a data set
    a = counter_helper(training_data)
    a_list = [t for t in a if t[0] == test_attri] #generate by the counter_helper
    a_list2 = [t[1] for t in a_list]
    list_C = []
    for c in a_list2:
        if c not in list_C: 
            list_C.append(c)
    
    class_list = [t[1] for t in training_data]
    classes_set = []
    for c in class_list:
        if c not in classes_set:
            classes_set.append(c)
     
    a_list3 = [t[1:] for t in a_list]
    num_s = []                          #list of number of each class under the attribute(test_attri) of each overall class
    for a in list_C:
        for c in classes_set:
            num_s.append((a, c, a_list3.count((a,c))))
    e_list = []
    S_list = []
    E_A = 0
    while len(num_s) !=0:
        temp_list = []
        for i in range(len(classes_set)):
            temp_list.append(num_s.pop(0))
        temp_list2 = [t[2] for t in temp_list]
        temp_list2 = np.array(temp_list2, dtype = float)
        num_S = np.sum(temp_list2, dtype = float)
        S_list.append(num_S)
        e_list.append(- np.sum(temp_list2/num_S * ma.log2(temp_list2/num_S)))
    num_total_S = np.sum(S_list, dtype = float)
    E_A =np.sum(S_list/num_total_S*e_list)
    #print(E_A)
    return E_A


# In[15]:


def total_info(training_data):                          
    num_C = [t[1] for t in counter_num_each_class(training_data)]
    num_C = np.array(num_C)
    num_S = np.sum(num_C)
    I_S = - np.sum(num_C/num_S * np.log2(num_C/num_S))
    return I_S


# In[16]:


def get_gain(training_data, test_attri):
    class_list = counter_helper(training_data)
    return total_info(training_data) - entropy(test_attri)


# In[31]:


def generator_DT(training_data):
    attribute_list = get_attribute_list(training_data)
    node = ""      # don't know the data structure of it yet
    branch = {}
    tree = (node,branch)

    # 2nd step of ID3
    for num in counter_num_each_class(training_data):      #checking number in each class
        if num[1] == len(training_data):               #if it equal to the length of the data set, means all samples in same class
            node = num[0]                                 #label it with that class
            branch = None                                  #indecate it as a leaf node

    # 3rd step of ID3
    if len(attribute_list) == 0:     #checking the length of the attribute
        node = most_common_class()      #label it with the most common class
        branch = None                    #indecate it as a leaf node

    # 4th step, Select test-attribute, the attribute among attribute-list with the highest information gain
    test_attri = attribute_list[0]  #  initialize a test attribute as the first element of attribute list
    max_gain = 0                    #  initialize a max information gain as 0
    for attri in attribute_list:
        if get_gain(training_data, test_attri) > max_gain:
            test_attri = attri      #  update the test_attri

    # 5th step, label node N with test attribute
    node = test_attri


    # 6th step6)	For each know value ai of test attribute:
    #i.	        Grow a branch from N for the condition test-attribute = ai
    #ii.	Let Si be the set of samples in the training set for which test-attribute = ai
    #iii.	If Si is empty, then attach a leaf node labelled with the most common class in the parent node
    #iv.	Else, attach a node returned by Generate-DT(Si , attribute-list â€“ test-attribute)

    values_list = values_in_test_attri(training_data, test_attri)  #get a list of every value of the test attribute
    common_in_parent = most_common_class(training_data)                         #need this in the for loop, could be done in the begining of the function???
    for value in values_list:
        newNode = ""
        branch = {value : newNode}  # create a branch

        sub_set_of_data = []
        for sample in training_data:
            if value in sample[0]:
                sub_set_of_data.append(sample)            # ii creating si
        if len(sub_set_of_data) == 0:                       # iii
            newNode = (common_in_parent,None)
        else:
            newNode = generator_DT(sub_set_of_data, attribute_list.remove(test_attri))     #iv 
            
    return tree


# In[32]:


generator_DT(training_data)

